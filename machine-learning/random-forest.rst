随机森林(Random Forest)
===========================
Overview
------------
随机森林产生很多的决策树，在判断一个新样本时，将样本喂给所有的树，每个树给出一个决策，最终整体的决策是这些决策的综合。

Tree bagging 方法
------------------
给定训练数据 :math:`\{x_i | i = 1, \cdots, N\}` 。

如果每棵树都喂给相同的数据，那么Forest中的树肯定是完全相同的了。 

bagging 方法中添加了一个随机性：

在训练一棵树的时候，有放回地抽取 :math:`N` 个样本（这意味着一些样本会重复）作为这棵树的训练数据。
训练这棵树，得到决策树 :math:`\hat{f}_b(X)` 。
如果，总共包含 :math:`B` 课树，那么最终采用bagging的方式综合决策：

.. math::

    \hat{f} = \frac{1}{B} \sum_{b=1}^B \hat{f}_b(X')

From bagging to random forests
------------------------------------
在Random Forest最初的论文里，作者认为Random Forest的效果与当中每个子树之间的相关度有关。
在bagging方法中，如果一个feature对整体效果贡献比较大，那么极有可能很多子树中，这个feature都处在比较靠近root的位置。

这个现象无疑表明，在bagging方法下，不同子树间的相关性比较高。

如何能够降低子树之间的相关度，
Random Forest采用如下方法：
在训练子树中，每个节点分割时，随机抽取整体feature的一个子集来做决策。
通过这个随机措施，来降低子树间的相关性。






