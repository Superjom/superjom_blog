===================
各种ML方法的优缺点
===================
.. sectionauthor:: Superjom <yanchunwei {AT} outlook.com>

|today|

分类器
---------

Naive Bayes
************
优点：

1. 对于小数据，简单轻便，不需要训练。

缺点：

1. 不适合大数据
2. 认为输入的feature各个维度之间是无关的，不能学习特征之间的相互作用


Logistic Regression
*********************
优点:

1. 比起SVM或者decision tree 原理易懂
2. 不用担心特征之间的相互作用
3. 便于在线学习，对后期的更大数据的加入

决策树
********

C4.5算法
++++++++++
基于信息增益：

.. math::

    g(D,A) = H(D) - H(D|A)

信息增益比:

.. math::

    g_R(D,A) = \frac{g(D,A)} {H(D)}

1. 在每一个节点，进行分支时 
   
    a) 扫描每一个feature，根据其取值（离散的）计算信息增益比
    b) 选择信息增益最大的feature进行拆分

2. 如果某个节点的信息增益比小于阀值，那么置 :math:`T` 为单节点树，投票得到唯一的类别

CART树
+++++++
回归树用平方误差最小化准则，分类树采用基尼系数

**回归树**

采用启发式的方法:

1. 用某个feature切分一个节点时，利用当前数据中，对于feature的取值尝试切分，贪心扫描得到某个点此feature的值作为切割值：

.. math::

    R_1(j, s) = \{x | x^{(j)} \lt s\}

    R_2(j, s) = \{x | x^{(j)} > s\}

对于某个切割点，具体的损失定义:

.. math::

    \min_{j,s} \left[
        \min_{c_1}
            \sum_{x_i \in R_1(j,s)}
                (y_i-c_i)^2 + 
        \min_{c_2} \sum_{x_i\in R_2(j,s)}
            (y_i - c_2)^2 \right]

拆分后，此节点上对应的值就是就是对应数据的平均值。


**分类树**

采用Gini系数来衡量纯度。

.. math::

    Gini(p) = \sum_{k=1}^K p_k (1-p_k) = 1- \sum_{k=1}^K p_k^2


优缺点
++++++++
优点：

1. 易于解释和说明
2. 非参数化，学习一个区域划分
3. 不用担心数据是否可以线性可分

缺点：

1. 不支持在线学习，有新的数据必须要重新建树
2. 容易过拟合（需要剪枝）

随机森林(Random Forest)
------------------------
随机森林可以认为是用一系列的小树去学习整个数据集。

其中每个小树的训练数据来自对训练集的随机抽样，如此，每棵树均只能得到部分数据。

而且，在树中的每个分支上，会随机抽取m个feature参与分类。


GBDT
-------
首先，boosting的意思就是，一系列的简单模型按照一定的顺序学习数据，其中后面的模型会尝试消除前面所有模型决策的错误，
最终的结果是所有的弱模型全体决策得出。

而Gradient boosting 的意思就是，弱模型学习的不是前面模型预测的误差，而是梯度。 
或者说，当前弱模型尝试着去将群体的误差向减小的方向上努力。

GBDT采用的弱模型是决策树。 

当然，决策树有过拟合的问题，GBDT通过两方面来避免单个树的过拟合：

1. 每个树只会随机得到部分的数据进行学习
2. 树的高度收到限制

可以认为GBDT 是效果比较好的模型。


KNN
-----
优点：

1. 简单，高效
2. 直观，容易解释
3. 适合于大数据
4. 对噪音robust


缺点：

1. 当大数据的性能不够好（可以通过特征选择解决）


SVM
-----
优点：

1. 有正则化项，通过确定参数来避免over-fitting.
2. 使用核，通过核可以融入人的知识和经验
3. SVM采用凸优化，并且有SMO等高效的运算方法
4. 有强大的理论支撑

缺点：

1. 核的选择及其他参数比较难，如果参数选择不好，容易导致overfiting.
2. 内存消耗大



References
-------------
`如何选择机器学习分类器 <http://www.open-open.com/news/view/1f66c10>`_

`随机森林 <http://www.zhizhihu.com/html/y2011/2743.html>`_

`A Gentle Introduction to Random Forests, Ensembles, and Performance Metrics in a Commercial System <http://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/>`_
